# DASHBOARD MONITORSSH

## TABLE DES MATIÃˆRES
1. [Contexte et Objectifs](#1-contexte-et-objectifs)
2. [Architecture Technique](#2-architecture-technique)
3. [FonctionnalitÃ©s AvancÃ©es : GÃ©olocalisation](#3-fonctionnalitÃ©s-avancÃ©es--gÃ©olocalisation)
4. [Code DÃ©taillÃ© et Optimisations](#4-code-dÃ©taillÃ©-et-optimisations)
5. [DÃ©ploiement et Production](#5-dÃ©ploiement-et-production)
6. [RÃ©sultats et MÃ©triques](#6-rÃ©sultats-et-mÃ©triques)
7. [AmÃ©liorations Futures](#7-amÃ©liorations-futures)

---

## 1. CONTEXTE ET OBJECTIFS

### ProblÃ©matique:
Les administrateurs systÃ¨me et les responsables sÃ©curitÃ© (CISO) font face Ã  un volume massif de logs SSH bruts. Ces donnÃ©es, bien que riches en information, sont :
- **Volumineuses** : +650 000 entrÃ©es Ã  traiter dans notre cas d'Ã©tude.
- **Abstraites** : Une liste d'adresses IP ne permet pas de visualiser l'origine gÃ©ographique des attaques.
- **Difficiles Ã  corrÃ©ler** : Impossible de lier rapidement une vague d'attaques Ã  un pays spÃ©cifique sans outil dÃ©diÃ©.

### Solution ApportÃ©e:
Une application **Web App interactive (SaaS)** complÃ¨te permettant de :
1. **IngÃ©rer** des logs SSH (CSV) de maniÃ¨re performante.
2. **Filtrer** dynamiquement les menaces (Temps, Type d'attaque, IP).
3. **GÃ©olocaliser** les attaquants sur une carte mondiale interactive.
4. **Visualiser** les tendances (Top Pays, Chronologie des attaques).
5. **DÃ©ployer** la solution publiquement pour un accÃ¨s universel.

### RÃ©sultat:
Une application professionnelle, stable, hÃ©bergÃ©e gratuitement sur Streamlit Cloud et accessible depuis n'importe quel navigateur web.


---


## 2. ARCHITECTURE TECHNIQUE

### 2.1 Stack Technologique

Le projet repose sur une architecture moderne orientÃ©e Data Science :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FRONTEND (Navigateur)                   â”‚
â”‚   - Streamlit UI (Sidebar, Graphiques, Tableaux)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           BACKEND (Streamlit Server)                    â”‚
â”‚  - Python 3.10+                                         â”‚
â”‚  - Pandas (Traitement de donnÃ©es)                       â”‚
â”‚  - Cache (@st.cache_data)                               â”‚
â”‚  - Altair/Map (Visualisation)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Lecture de fichiers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STOCKAGE (GitHub + Cloud)                  â”‚
â”‚  - app.py, requirements.txt (GitHub)                    â”‚
â”‚  - datasetssh.csv (DÃ©pÃ´t GitHub)                        â”‚
â”‚  - DÃ©ploiement via Streamlit Cloud                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Gestion AvancÃ©e du Cache (`@st.cache_data`)

**Le cache est crucial pour la performance.**

Sans cache :

- Ã€ chaque clic sur un filtre â†’ Rechargement du CSV (655k lignes)
- **Temps d'attente :** 2-3 secondes par interaction
- **ExpÃ©rience utilisateur :** Frustrante

Avec cache (@st.cache_data) :

L'optimisation est critique pour deux aspects :
1.  **Chargement des donnÃ©es** : Ã‰vite de recharger le CSV (75 Mo) Ã  chaque interaction utilisateur.
2.  **GÃ©olocalisation API** : Stocke les coordonnÃ©es GPS en mÃ©moire pour ne pas interroger l'API externe inutilement Ã  chaque rafraÃ®chissement.
3.  **Gain de performance** : Passage de ~20 secondes (appel API initial) Ã  <100ms (lecture cache).
4.  **ExpÃ©rience utilisateur :** Fluide et responsive

---

## 3. FONCTIONNALITÃ‰S AVANCÃ‰ES : GÃ‰OLOCALISATION

### 3.1 MÃ©canisme de GÃ©olocalisation
L'application enrichit les logs bruts en interrogeant une API externe pour convertir les adresses IP en coordonnÃ©es gÃ©ographiques.

*   **API utilisÃ©e** : `ip-api.com` (Endpoint Batch).
*   **Contrainte technique** : Limite stricte de 45 requÃªtes/minute et 100 IPs par requÃªte.
*   **StratÃ©gie implÃ©mentÃ©e** :
    1.  Extraction des IPs uniques uniquement.
    2.  DÃ©coupage des IPs en lots (batchs) de 100.
    3.  **Temporisation automatique (`time.sleep(1.5)`)** entre les lots pour respecter le rate-limiting et Ã©viter le bannissement de l'IP du serveur.
    4.  Traitement des erreurs robuste (`try/except`) pour garantir la stabilitÃ© de l'application.

### 3.2 Visualisation
*   **Carte Interactive (`st.map`)** : Projection des points d'attaques (Lat/Lon) sur une carte mondiale.
*   **Graphique Top Pays (Altair)** : Diagramme en barres triÃ© automatiquement par volume dÃ©croissant pour identifier instantanÃ©ment les principaux pays sources, remplaÃ§ant le tri alphabÃ©tique par dÃ©faut.

---

## 4. CODE DÃ‰TAILLÃ‰ ET OPTIMISATIONS

### 4.1 Configuration et Imports:

```python
import streamlit as st # Streamlit est le framework qui crÃ©e l'interface web. Il gÃ¨re automatiquement la conversion de code Python en UI interactive.
import pandas as pd    # Pandas est la librairie standard pour manipuler les donnÃ©es (DataFrames). Un DataFrame est comme une table Excel : lignes + colonnes
import requests        # Pour interroger l'API de gÃ©olocalisation
import os              # Permet au script Python de "discuter" avec l'ordinateur sur lequel il tourne pour faire des tÃ¢ches
import time            # Pour gÃ©rer les pauses (rate limiting)
import altair as alt   # Pour les graphiques avancÃ©s (Top Pays)

script_dir = os.path.dirname(__file__) # RÃ©cupÃ¨re le chemin du dossier actuel
```
### 4.2 Configuration de la Page Streamlit:

```python
# ======== CONFIGURATION DE PAGE ========
st.set_page_config(
    # POINT CRUCIAL : Cette fonction DOIT Ãªtre appelÃ©e avant tout autre code Streamlit !
    page_title="MonitorSSH",
    # DÃ©finit le titre qui apparaÃ®t dans l'onglet du navigateur
    # Exemple dans la barre du navigateur : [MonitorSSH] â† C'est Ã§a
    
    page_icon="ğŸ”’",
    # DÃ©finit l'emoji qui apparaÃ®t dans l'onglet du navigateur (c'est cosmÃ©tique mais professionnel)
    
    layout="wide"
    # "wide" = utilise toute la largeur de l'Ã©cran (mieux pour les graphiques larges)
    # Alternative : "centered" = contenu centrÃ© (moins de place)
)
```

### 4.3 Fonction de Chargement (ETL):

```python
# ======== DÃ‰CORATEUR CACHE - TRÃˆS IMPORTANT ========
@st.cache_data
# Ce dÃ©corateur dit Ã  Streamlit : "Sauvegarde le rÃ©sultat de cette fonction en mÃ©moire"
# Si on l'appelle avec les mÃªmes paramÃ¨tres, retourne la version en cache (pas de rechargement)

def load_data(file_path_or_buffer):
    # ParamÃ¨tre "file_path_or_buffer" = flexibilitÃ© totale
    # Peut Ãªtre soit :
    #   - Un string : 'datasetssh.csv' (chemin local)
    #   - Un objet fichier : uploaded_file (fichier uploadÃ© par l'utilisateur)
    
    # ======== LIGNE 1 : CHARGEMENT CSV ========
    df = pd.read_csv(file_path_or_buffer)
    # pd.read_csv() lit un fichier CSV et le convertit en DataFrame
    # DataFrame = structure de donnÃ©es 2D (comme une table SQL)
    # Exemple :
    # | Timestamp        | EventId | SourceIP | User | Raw_Message |
    # |------------------|---------|----------|------|-------------|
    # | 2024-12-10...    | E27     | 173.2... | None | reverse ... |
    
    # ======== LIGNE 2 : VÃ‰RIFICATION COLONNE ========
    if 'Timestamp' in df.columns:
        # VÃ©rifier que la colonne 'Timestamp' existe
        # Si elle n'existe pas, on ne peut pas faire l'analyse temporelle
        
        # ======== LIGNE 3 : CONVERSION DE DATES ========
        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')
        # Convertit la colonne texte '2024-12-10 06:55:46' en objet datetime
        # errors='coerce' = si une date est mal formÃ©e, la remplacer par NaT (Not a Time)
        # Pourquoi important ? Permet les comparaisons : date1 > date2 (sinon erreur)
        
    else:
        # Si colonne Timestamp n'existe pas
        st.error("Erreur: Le fichier CSV doit contenir une colonne 'Timestamp'.")
        # Affiche un message d'erreur en rouge dans l'interface
        return pd.DataFrame()
        # Retourne un DataFrame vide (signale une erreur)
    
    # ======== LIGNE 4 : RETOUR ========
    return df
    # Retourne le DataFrame nettoyÃ© et prÃªt Ã  l'emploi
```

### 4.4 Module de GÃ©olocalisation:

Ce code gÃ¨re la communication avec l'API externe tout en respectant les limites de dÃ©bit.

```python
@st.cache_data # Stocke les rÃ©sultats en mÃ©moire pour Ã©viter de payer le temps d'attente 2 fois

def get_locations(ip_list):
    """
    RÃ©cupÃ¨re lat/lon/pays pour une liste d'IPs uniques.
    GÃ¨re le rate-limiting et le batch processing.
    """
    locations = []
    unique_ips = list(set(ip_list)) # On enlÃ¨ve les doublons

    # Traitement par paquets de 100 pour optimiser les appels rÃ©seau
    for i in range(0, len(unique_ips), 100):
        batch = unique_ips[i:i+100]
        try:
            # Appel API Batch (1 requÃªte = 100 IPs)
            response = requests.post(
                "http://ip-api.com/batch", 
                json=[{"query": ip, "fields": "lat,lon,country,query"} for ip in batch]
             ).json()
        
            # Parsing de la rÃ©ponse et Stockage des rÃ©sultats
            for item in response:
                if 'lat' in item and 'lon' in item:
                  locations.append({
                        'ip': item['query'], 
                       'lat': item['lat'], 
                       'lon': item['lon'],
                       'country': item.get('country', 'Inconnu')
                    })
        
            # PAUSE DE SÃ‰CURITÃ‰ (Rate Limiting)
            # Indispensable pour Ã©viter le bannissement API
            time.sleep(1.5) 
        
        except Exception as e:
            st.error(f"Erreur API : {e}")
            break
        
    return pd.DataFrame(locations)
```

### 4.5 Tri et Affichage AvancÃ© (Altair):

Utilisation de la librairie Altair pour forcer l'ordre dÃ©croissant des barres (le comportement par dÃ©faut de Streamlit Ã©tant parfois alphabÃ©tique).

```python
# Utilisation de la librairie Altair pour un contrÃ´le total
chart = alt.Chart(top_countries).mark_bar().encode(
x=alt.X('Pays', sort='-y', title='Pays'), # Tri forcÃ© sur l'axe Y (Volume)
y=alt.Y('Nombre', title="Nombre d'attaques"),
tooltip=['Pays', 'Nombre']
)
st.altair_chart(chart, use_container_width=True)
```

### 4.6 Fonction Principale (Interface):

```python
# ======== DÃ‰FINITION DE LA FONCTION PRINCIPALE ========
def main():
    # Toute la logique de l'interface est encapsulÃ©e dans cette fonction
    # C'est un pattern propre en Python
    
    # ======== TITRE ========
    st.title("ğŸ”’ Dashboard de SÃ©curitÃ© : Clinique Tamalou")
    # CrÃ©e un titre de niveau 1 (Ã©quivalent Ã  <h1> en HTML)
    # L'emoji ğŸ”’ donne un aspect "sÃ©curitÃ©"
    
    # ======== SIDEBAR - UPLOAD DE FICHIER ========
    st.sidebar.header("ğŸ“ DonnÃ©es")
    # "st.sidebar" = tout ce qui suit apparaÃ®t dans la barre latÃ©rale gauche
    # header() = titre de section
    
    uploaded_file = st.sidebar.file_uploader(
        "Charger un nouveau fichier CSV",
        # Label du bouton
        type=['csv']
        # Restriction : seuls les fichiers .csv sont acceptÃ©s
    )
    # file_uploader() retourne un objet fichier (ou None si rien n'est uploadÃ©)
    
    # ======== LOGIQUE DE CHARGEMENT ========
    if uploaded_file is not None:
        # Si l'utilisateur A uploadÃ© un fichier personnalisÃ©
        st.sidebar.success("Fichier personnalisÃ© chargÃ© !")
        # Message de succÃ¨s (en vert)
        df_brut = load_data(uploaded_file)
        # Charge le fichier uploadÃ©
        
    else:
        # Si l'utilisateur N'a RIEN uploadÃ©
        try:
            # "try" = essayer d'exÃ©cuter le code suivant
            # "except" = si une erreur surgit, faire quelque chose
            
            df_brut = load_data('datasetssh.csv')
            # Charge le fichier de dÃ©mo par dÃ©faut
            st.sidebar.info("Utilisation du fichier de dÃ©mo par dÃ©faut.")
            # Message informatif (en bleu)
            
        except FileNotFoundError:
            # Si 'datasetssh.csv' n'existe pas
            st.error("Fichier de dÃ©mo 'datasetssh.csv' introuvable.")
            return
            # ArrÃªte l'exÃ©cution (pas de donnÃ©es = pas d'interface)
    
    # ======== SÃ‰CURITÃ‰ : DATAFRAME VIDE ? ========
    if df_brut.empty:
        # Si le DataFrame est vide (pas une seule ligne)
        return
        # ArrÃªte tout (erreur critique)
    
    # ======== SIDEBAR - FILTRES ========
    st.sidebar.header("Filtres")
    # Nouveau titre de section dans la sidebar
    
    # -------- FILTRE 1 : DATES --------
    # On rÃ©cupÃ¨re les dates min/max du dataset pour les bornes
    min_date = df_brut['Timestamp'].min()
    # min() retourne la date la plus ANCIENNE
    max_date = df_brut['Timestamp'].max()
    # max() retourne la date la plus RÃ‰CENTE
    
    start_date = st.sidebar.date_input(
        "Date de dÃ©but",
        # Label
        min_date,
        # Valeur par dÃ©faut (premiÃ¨re date du dataset)
        min_value=min_date,
        # L'utilisateur ne peut pas aller AVANT cette date
        max_value=max_date
        # L'utilisateur ne peut pas aller APRÃˆS cette date
    )
    # Retourne un objet date (ex : datetime.date(2024, 12, 10))
    
    end_date = st.sidebar.date_input(
        "Date de fin",
        max_date,
        min_value=min_date,
        max_value=max_date
    )
    
    # -------- FILTRE 2 : EVENT ID (Type d'attaque) --------
    all_event_ids = df_brut['EventId'].unique()
    # unique() retourne les valeurs UNIQUES (sans doublon) de la colonne
    # Exemple : ['E27', 'E13', 'E12', 'E21', ...] (environ 10 types d'Ã©vÃ©nements)
    
    selected_events = st.sidebar.multiselect(
        "SÃ©lectionner les EventId",
        # Label
        all_event_ids,
        # Liste des options disponibles
        default=all_event_ids
        # Par dÃ©faut, TOUS les EventId sont sÃ©lectionnÃ©s
    )
    # Retourne une LISTE des valeurs cochÃ©es par l'utilisateur
    # Exemple : ['E27', 'E13', 'E12'] (l'utilisateur a dÃ©crochÃ© E21)
    
    # -------- FILTRE 3 : IP SPÃ‰CIFIQUE --------
    all_ips = df_brut['SourceIP'].unique()
    # Liste de toutes les IPs du dataset (ex: ['173.234.31.186', '183.129.154.138', ...])
    
    selected_ip = st.sidebar.selectbox(
        "Rechercher une IP spÃ©cifique",
        # Label
        options=["Toutes"] + list(all_ips)
        # OPTIONS = ["Toutes", '173.234.31.186', '183.129.154.138', ...]
        # "Toutes" permet de ne PAS filtrer par IP
    )
    # Retourne UNE SEULE valeur sÃ©lectionnÃ©e par l'utilisateur (pas une liste)
    # Exemple : "173.234.31.186" ou "Toutes"
    
    # ======== APPLICATION DES FILTRES ========
    # CrÃ©er des "masques boolÃ©ens" (True/False pour chaque ligne)
    
    # -------- MASQUE 1 : DATES --------
    mask_date = (df_brut['Timestamp'].dt.date >= start_date) & (df_brut['Timestamp'].dt.date <= end_date)
    # df_brut['Timestamp'].dt.date = extrait JUSTE la date (pas l'heure)
    # >= start_date : lignes APRÃˆS la date de dÃ©but
    # <= end_date : lignes AVANT la date de fin
    # & = ET logique (les deux conditions doivent Ãªtre vraies)
    # Exemple :
    #   Ligne 1 : Timestamp = 2024-12-10, start = 2024-12-10, end = 2025-01-07 â†’ TRUE (incluse)
    #   Ligne 2 : Timestamp = 2024-12-09, start = 2024-12-10, end = 2025-01-07 â†’ FALSE (exclue)
    
    # -------- MASQUE 2 : EVENEMENT --------
    mask_event = df_brut['EventId'].isin(selected_events)
    # isin() = "est dans la liste ?"
    # Exemple :
    #   Ligne 1 : EventId = E27, selected_events = ['E27', 'E13'] â†’ TRUE (E27 est dans la liste)
    #   Ligne 2 : EventId = E21, selected_events = ['E27', 'E13'] â†’ FALSE (E21 n'est pas dans la liste)
    
    # -------- COMBINATION DES DEUX MASQUES --------
    df_filtered = df_brut[mask_date & mask_event]
    # & = ET logique : la ligne doit respecter BOTH conditions pour Ãªtre incluse
    # Exemple :
    #   Ligne 1 : mask_date=TRUE, mask_event=TRUE â†’ incluse
    #   Ligne 2 : mask_date=TRUE, mask_event=FALSE â†’ exclue
    #   Ligne 3 : mask_date=FALSE, mask_event=TRUE â†’ exclue
    
    # -------- MASQUE 3 : IP (OPTIONNEL) --------
    if selected_ip != "Toutes":
        # Si l'utilisateur A sÃ©lectionnÃ© une IP spÃ©cifique (pas "Toutes")
        df_filtered = df_filtered[df_filtered['SourceIP'] == selected_ip]
        # Filtre ENCORE le DataFrame pour garder que cette IP
    
    # Ã€ ce stade, df_filtered contient UNIQUEMENT les lignes qui respectent TOUS les filtres
    
    # ======== AFFICHAGE DES RÃ‰SULTATS ========
    st.markdown("---")
    # Affiche une ligne horizontale (sÃ©parateur visuel)
    
    # -------- SÃ‰CURITÃ‰ : TABLEAU VIDE ? --------
    if df_filtered.empty:
        st.warning("âš ï¸ Aucune donnÃ©e ne correspond Ã  vos filtres.")
        # Si aucune ligne ne correspond, affiche un message d'alerte (jaune)
    else:
        # Sinon (si des donnÃ©es existent)
        
        # -------- KPIs (INDICATEURS CLÃ‰S) --------
        col1, col2, col3 = st.columns(3)
        # CrÃ©e 3 colonnes de largeur Ã©gale cÃ´te Ã  cÃ´te
        
        with col1:
            # Contenu DANS la premiÃ¨re colonne
            st.metric(
                "Total Logs (FiltrÃ©s)",
                # Label
                df_filtered.shape[0]
                # shape[0] = nombre de LIGNES
                # Exemple : 655147
            )
        
        with col2:
            # Contenu DANS la deuxiÃ¨me colonne
            pourcentage = (len(df_filtered) / len(df_brut)) * 100
            # len() = compte le nombre de lignes
            # len(df_filtered) / len(df_brut) = ratio
            # * 100 = conversion en pourcentage
            # Exemple : 655147 / 655147 * 100 = 100.0%
            st.metric("% du Dataset", f"{pourcentage:.1f}%")
            # f"...{pourcentage:.1f}%" = formatage chaÃ®ne
            # :.1f = affiche 1 seul chiffre aprÃ¨s la virgule
            # Exemple : 100.0% (pas 100.000001%)
        
        with col3:
            # Contenu DANS la troisiÃ¨me colonne
            st.metric(
                "IPs Uniques",
                df_filtered['SourceIP'].nunique()
                # nunique() = nombre de VALEURS UNIQUES
                # Exemple : 1129 IPs diffÃ©rentes
            )
        
        # -------- TABLEAU DE DONNÃ‰ES --------
        st.subheader("ğŸ“‹ Logs FiltrÃ©s")
        # Sous-titre
        st.dataframe(
            df_filtered,
            # Le DataFrame Ã  afficher
            use_container_width=True
            # Le tableau utilise 100% de la largeur disponible
        )
        # Affiche un tableau interactif (scrollable, triable par colonne)
        
        # -------- VISUALISATIONS --------
        st.markdown("---")
        st.header("ğŸ“Š Analyse Visuelle")
        
        # CrÃ©e 2 colonnes pour 2 graphiques cÃ´te Ã  cÃ´te
        chart_col1, chart_col2 = st.columns(2)
        
        # -------- GRAPHIQUE 1 : TOP 10 IPS --------
        with chart_col1:
            st.subheader("Top 10 IPs Attaquantes")
            top_ips = df_filtered['SourceIP'].value_counts().head(10)
            # value_counts() = compte les occurrences de chaque valeur
            # Exemple rÃ©sultat :
            #   173.234.31.186      45000
            #   183.129.154.138     40000
            #   ...
            # head(10) = garder seulement les 10 premiÃ¨res
            st.bar_chart(top_ips)
            # Affiche un bar chart (graphique en barres)
        
        # -------- GRAPHIQUE 2 : TIME SERIES --------
        with chart_col2:
            st.subheader("Volume d'attaques par Heure")
            time_series = df_filtered.set_index('Timestamp').resample('H').size()
            # set_index('Timestamp') = utilise Timestamp comme index (pour le regroupement temporel)
            # resample('H') = regroupe par HEURE (H = hour)
            # .size() = compte le nombre de lignes dans chaque groupe
            # Exemple rÃ©sultat :
            #   2024-12-10 00:00:00    1200 (1200 attaques entre 00h et 01h)
            #   2024-12-10 01:00:00    950
            #   ...
            st.line_chart(time_series)
            # Affiche un line chart (graphique en courbe)
        
        # -------- GRAPHIQUE 3 : TOP USERNAMES --------
        st.markdown("---")
        st.subheader("ğŸš¨ Top Usernames TentÃ©s")
        top_users = df_filtered['User'].value_counts().head(10)
        # MÃªme logique que top_ips (mais avec les noms d'utilisateurs)
        # Exemple : root (400k tentatives), admin (5k), support (200), ...
        st.bar_chart(top_users)

# ======== GÃ‰OLOCALISATION ========
    st.markdown("---")
    st.header("ğŸŒ Carte des Attaques")

    if 'SourceIP' in df_filtered.columns:
        ips_to_locate = df_filtered['SourceIP'].dropna().unique().tolist()
        
        if len(ips_to_locate) > 0:
            st.info(f"GÃ©olocalisation de {len(ips_to_locate)} adresses IP uniques en cours...")
            
            with st.spinner("Interrogation de l'API de localisation..."):
                df_locations = get_locations(ips_to_locate)

            if not df_locations.empty:
                # 1. LA CARTE
                st.map(df_locations, size=20, color='#FF0000')
                st.caption(f"{len(df_locations)} localisations trouvÃ©es.")
                
                # 2. LE GRAPHIQUE TOP PAYS (ALTAIR)
                st.subheader("ğŸ“Š Top 10 des Pays d'origine")
                
                # PrÃ©paration des donnÃ©es
                top_countries = df_locations['country'].value_counts().head(10).reset_index()
                top_countries.columns = ['Pays', 'Nombre']
                
                # CrÃ©ation du graphique Altair (Tri dÃ©croissant forcÃ©)
                chart = alt.Chart(top_countries).mark_bar().encode(
                    x=alt.X('Pays', sort='-y', title='Pays'), # Trie l'axe X selon les valeurs de Y dÃ©croissantes
                    y=alt.Y('Nombre', title="Nombre d'attaques"),
                    tooltip=['Pays', 'Nombre']
                ).properties(height=400)
                
                st.altair_chart(chart, use_container_width=True)

            else:
                st.warning("Aucune localisation trouvÃ©e.")

# ======== POINT D'ENTRÃ‰E ========
if __name__ == "__main__":
    # Cette condition = "si ce fichier est lancÃ© directement (pas importÃ© ailleurs)"
    main()
    # Appelle la fonction principale
```
---

## 5. DÃ‰PLOIEMENT ET PRODUCTION

### Environnement:

*   **Plateforme** : Streamlit Community Cloud.
*   **Source** : DÃ©pÃ´t GitHub connectÃ© en CI/CD (Continuous Deployment).

```
ssh_monitor/
â”œâ”€â”€ app.py                 # Fichier principal Streamlit
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ .gitignore            # Fichiers Ã  ignorer lors du push GitHub
â””â”€â”€ datasetssh.csv        # DonnÃ©es de dÃ©mo
```

*   **Fichiers de configuration** :

**`requirements.txt`** (DÃ©pendances) :
```
streamlit
pandas
requests
altair
```
Contient les librairies nÃ©cessaires. Ã€ installer avec : `pip install -r requirements.txt`

**`.gitignore`**
```
.venv/
__pycache__/
*.pyc
.DS_Store
```
Indique Ã  Git quels fichiers IGNORER lors du commit (Ã©vite de versionner l'environnement virtuel lourd).

**Commandes Git (dans le dossier ssh_monitor) :**

```bash
# 1. Initialiser Git localement
git init
# CrÃ©e un dÃ©pÃ´t Git cachÃ© (.git)

# 2. Ajouter tous les fichiers
git add .
# Stocke les fichiers modifiÃ©s en "staging area"

# 3. CrÃ©er un commit (snapshot)
git commit -m "Ajout Bonus Upload + Version Finale"
# Sauvegarde les changements avec un message descriptif

# 4. Envoyer sur GitHub
git push
# Synchronise le dÃ©pÃ´t local avec GitHub (si un remote est configurÃ©)
```
**DÃ©ploiement sur Streamlit Cloud**

1. Aller sur https://share.streamlit.io/
2. Cliquer "New app"
3. Remplir le formulaire :
   - **Repository :** Yassine-Bouzidi/Analyse-Logs-SSH
   - **Branch :** main
   - **Main file path :** project/ssh_monitor/app.py
   - **App URL :** dashboard-ssh-ysn
4. Cliquer "Deploy!"

**RÃ©sultat :** L'app est en ligne Ã  https://dashboard-ssh-ysn.streamlit.app

**DÃ©ploiement Continu (CI/CD)**

Chaque fois que vous faites `git push` :
1. GitHub reÃ§oit le nouveau code
2. Streamlit Cloud dÃ©tecte le changement (via webhook)
3. L'app est reconstruite automatiquement (environ 1 min)
4. La version en ligne se met Ã  jour

C'est l'avantage du dÃ©ploiement continu : zÃ©ro downtime, mise Ã  jour instantanÃ©e.

### Cycle de Mise Ã  Jour:
1.  Modification du code en local (VSCode).
2.  Test local (`streamlit run app.py`).
3.  Push vers GitHub (`git push`).
4.  Re-dÃ©ploiement automatique par Streamlit Cloud (ZÃ©ro maintenance serveur).

---

## 6. RÃ‰SULTATS ET MÃ‰TRIQUES

### Performance Technique:
*   **Volume traitÃ©** : CapacitÃ© Ã  gÃ©rer +1000 IPs uniques pour la gÃ©olocalisation.
*   **StabilitÃ© API** : 100% de rÃ©ussite grÃ¢ce Ã  la gestion des pauses (`Sleep`).
*   **Temps de rÃ©ponse** : ~20s pour la premiÃ¨re gÃ©olocalisation complÃ¨te (1129 IPs), **immÃ©diat** (<100ms) pour les affichages suivants grÃ¢ce au cache.

### Insights SÃ©curitÃ©:
*   **Top Pays** : Identification claire des sources majeures (ex: Chine, USA, CorÃ©e du Sud).
*   **CorrÃ©lation** : La carte permet de distinguer une attaque ciblÃ©e (un seul point gÃ©ographique) d'une attaque par Botnet distribuÃ© (points multiples).
*   **Filtrage** : CapacitÃ© Ã  isoler une IP spÃ©cifique et voir instantanÃ©ment son pays d'origine et son historique d'attaques.

---

## 7. AMÃ‰LIORATIONS FUTURES 

1.  **API Key PrivÃ©e** : Passer sur une version payante de l'API de gÃ©olocalisation pour supprimer la latence de 1.5s et permettre le temps rÃ©el.
2.  **Enrichissement ASN** : Ajouter l'information du fournisseur d'accÃ¨s (ISP) pour savoir si l'attaque vient d'un hÃ©bergeur (AWS, OVH) ou d'une connexion rÃ©sidentielle.
3.  **Mode Sombre/Clair** : AmÃ©liorer l'accessibilitÃ© de l'interface utilisateur.
4. **Export PDF** : Bouton pour tÃ©lÃ©charger un rapport filtrÃ©
5. **Authentification** : SÃ©curiser l'accÃ¨s avec login/password (via `st.secrets`)
6. **Real-time Updates** : IntÃ©gration avec une API pour les logs live
7. **Alertes** : Notifications email si une IP dÃ©passe X tentatives
8. **Machine Learning** : DÃ©tection d'anomalies (comportement anormal)
9. **Base de donnÃ©es** : Passer de CSV Ã  PostgreSQL pour plus de performance
10. **ScalabilitÃ©** : Migrer de Streamlit Cloud vers Kubernetes (si trafic augmente)
11. **Backup** : Sauvegardes automatiques des logs en cloud
---

## 8. CONCLUSION

Ce projet dÃ©montre une **maÃ®trise complÃ¨te du cycle de dÃ©veloppement** :
- âœ… Analyse de donnÃ©es (Pandas, ETL)
- âœ… DÃ©veloppement d'interface (Streamlit)
- âœ… DÃ©ploiement en production (GitHub, Cloud)
- âœ… Optimisation (Cache, Performance)

---

## ANNEXES

### Glossaire Technique: 
- **ETL** : Extract (extraire), Transform (transformer), Load (charger)
- **DataFrame** : Table de donnÃ©es en mÃ©moire (colonne + lignes)
- **Cache** : Stockage temporaire en mÃ©moire pour Ã©viter recalcul
- **Masque boolÃ©en** : Tableau True/False pour filtrer les lignes
- **CI/CD** : Continuous Integration / Continuous Deployment (automatisation)
- **SOC** : Security Operations Center (Ã©quipe de sÃ©curitÃ©)

---