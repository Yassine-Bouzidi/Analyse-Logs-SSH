# RAPPORT TECHNIQUE COMPLET : MONITORSSH
## Dashboard de SÃ©curitÃ© - Analyse de Logs SSH

---

**Projet :** MonitorSSH - Web App de Monitoring SSH  
**Auteur :** Yassine Bouzidi  
**Date :** 8 DÃ©cembre 2025  
**Formation :** Simplon - CybersÃ©curitÃ©  
**Technologie :** Python, Streamlit, Pandas, GitHub, Streamlit Cloud  
**Lien Public :** https://dashboard-ssh-ysn.streamlit.app  
**DÃ©pÃ´t GitHub :** https://github.com/Yassine-Bouzidi/Analyse-Logs-SSH

---

## TABLE DES MATIÃˆRES
1. Contexte et Objectifs
2. Architecture Technique
3. Ã‰tapes de DÃ©veloppement (Jour 1 & Jour 2)
4. Code DÃ©taillÃ© avec Explications
5. DÃ©ploiement et Production
6. RÃ©sultats et MÃ©triques
7. AmÃ©liorations Futures

---

## 1. CONTEXTE ET OBJECTIFS

### ProblÃ©matique
Les administrateurs systÃ¨me et les responsables sÃ©curitÃ© (CISO) reÃ§oivent quotidiennement des centaines de milliers de logs SSH bruts. Ces donnÃ©es sont :
- **Non structurÃ©es** : texte brut, difficile Ã  parcourir
- **Ã‰normes** : 655 147 entrÃ©es dans notre cas
- **Inutilisables** : sans outils de visualisation adÃ©quats

### Solution ApportÃ©e
DÃ©velopper une **Web App interactive** (SaaS) permettant de :
1. Charger des fichiers de logs SSH en format CSV
2. Filtrer les donnÃ©es par plusieurs critÃ¨res (dates, types d'Ã©vÃ©nements, adresses IP)
3. Visualiser les donnÃ©es sous forme de graphiques et tableaux
4. Identifier rapidement les menaces et les patterns d'attaque
5. DÃ©ployer publiquement pour accÃ¨s multi-utilisateurs

### RÃ©sultat
Une application professionnelle, stable, hÃ©bergÃ©e gratuitement sur Streamlit Cloud et accessible depuis n'importe quel navigateur web.

---

## 2. ARCHITECTURE TECHNIQUE

### 2.1 Stack Technologique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FRONTEND (Navigateur)                    â”‚
â”‚        - Streamlit UI (Sidebar, Graphiques, Tableaux)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           BACKEND (Streamlit Server)                     â”‚
â”‚  - Python 3.10+                                          â”‚
â”‚  - Pandas (Traitement de donnÃ©es)                        â”‚
â”‚  - Cache (@st.cache_data)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Lecture de fichiers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STOCKAGE (GitHub + Cloud)                   â”‚
â”‚  - app.py, requirements.txt (GitHub)                     â”‚
â”‚  - datasetssh.csv (DÃ©pÃ´t GitHub)                         â”‚
â”‚  - DÃ©ploiement via Streamlit Cloud                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Fonctionnement du Cache (@st.cache_data)

**Le cache est crucial pour la performance.**

Sans cache :
- Ã€ chaque clic sur un filtre â†’ Rechargement du CSV (655k lignes)
- **Temps d'attente :** 2-3 secondes par interaction
- **ExpÃ©rience utilisateur :** Frustrante

Avec cache (@st.cache_data) :
- Premier chargement : Sauvegarde le DataFrame en mÃ©moire
- Interactions suivantes : Lecture depuis la RAM (instantanÃ©)
- **Temps d'attente :** <100ms
- **ExpÃ©rience utilisateur :** Fluide et responsive

---

## 3. Ã‰TAPES DE DÃ‰VELOPPEMENT

### JOUR 1 : Architecture et Premiers Affichages

#### Ã‰tape 1 : PrÃ©paration du Projet
```
ssh_monitor/
â”œâ”€â”€ app.py                 # Fichier principal Streamlit
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ .gitignore            # Fichiers Ã  ignorer lors du push GitHub
â””â”€â”€ datasetssh.csv        # DonnÃ©es de dÃ©mo
```

**Fichiers crÃ©Ã©s :**

**1. requirements.txt**
```
streamlit
pandas
matplotlib
```
Contient les librairies nÃ©cessaires. Ã€ installer avec : `pip install -r requirements.txt`

**2. .gitignore**
```
.venv/
__pycache__/
*.pyc
.DS_Store
```
Indique Ã  Git quels fichiers IGNORER lors du commit (Ã©vite de versionner l'environnement virtuel lourd).

#### Ã‰tape 2 : Code Initial (app.py)
**Voir section 4 pour explications ligne par ligne.**

#### Ã‰tape 3 : Lancement Local
```bash
streamlit run app.py
```
L'app s'ouvre automatiquement sur `localhost:8501`.

---

### JOUR 2 : InteractivitÃ© et Production

#### Phase 1 : Ajout des Filtres (Sidebar)
- Filtre par plage de dates
- Filtre par type d'Ã©vÃ©nement (multiselect)
- Filtre par IP spÃ©cifique

#### Phase 2 : Visualisation (Graphiques)
- Bar Chart : Top 10 IPs attaquantes
- Line Chart : Volume d'attaques par heure
- Bar Chart : Usernames les plus tentÃ©s

#### Phase 3 : DÃ©ploiement (GitHub + Streamlit Cloud)
1. Initialiser Git : `git init`
2. Commit initial : `git commit -m "Initial commit"`
3. Push GitHub : `git push`
4. DÃ©ployer sur Streamlit Cloud (lien du repo)
5. App publique en ligne (URL `dashboard-ssh-ysn.streamlit.app`)

#### Phase 4 : Bonus (Upload de fichier)
Ajout de la fonctionnalitÃ© `st.file_uploader` pour permettre aux utilisateurs de charger leurs propres fichiers CSV.

---

## 4. CODE DÃ‰TAILLÃ‰ AVEC EXPLICATIONS

### 4.1 Configuration et Imports

```python
# ======== LIGNE 1 : IMPORT STREAMLIT ========
import streamlit as st
# Streamlit est le framework qui crÃ©e l'interface web.
# Il gÃ¨re automatiquement la conversion de code Python en UI interactive.
# Alternative : Flask/Django (plus lourd)

# ======== LIGNE 2 : IMPORT PANDAS ========
import pandas as pd
# Pandas est la librairie standard pour manipuler les donnÃ©es (DataFrames).
# Un DataFrame est comme une table Excel : lignes + colonnes
```

### 4.2 Configuration de la Page Streamlit

```python
# ======== CONFIGURATION DE PAGE ========
st.set_page_config(
    # POINT CRUCIAL : Cette fonction DOIT Ãªtre appelÃ©e avant tout autre code Streamlit !
    page_title="MonitorSSH",
    # DÃ©finit le titre qui apparaÃ®t dans l'onglet du navigateur
    # Exemple dans la barre du navigateur : [MonitorSSH] â† C'est Ã§a
    
    page_icon="ğŸ”’",
    # DÃ©finit l'emoji qui apparaÃ®t dans l'onglet du navigateur (c'est cosmÃ©tique mais professionnel)
    
    layout="wide"
    # "wide" = utilise toute la largeur de l'Ã©cran (mieux pour les graphiques larges)
    # Alternative : "centered" = contenu centrÃ© (moins de place)
)
```

### 4.3 Fonction de Chargement (ETL)

```python
# ======== DÃ‰CORATEUR CACHE - TRÃˆS IMPORTANT ========
@st.cache_data
# Ce dÃ©corateur dit Ã  Streamlit : "Sauvegarde le rÃ©sultat de cette fonction en mÃ©moire"
# Si on l'appelle avec les mÃªmes paramÃ¨tres, retourne la version en cache (pas de rechargement)

def load_data(file_path_or_buffer):
    # ParamÃ¨tre "file_path_or_buffer" = flexibilitÃ© totale
    # Peut Ãªtre soit :
    #   - Un string : 'datasetssh.csv' (chemin local)
    #   - Un objet fichier : uploaded_file (fichier uploadÃ© par l'utilisateur)
    
    # ======== LIGNE 1 : CHARGEMENT CSV ========
    df = pd.read_csv(file_path_or_buffer)
    # pd.read_csv() lit un fichier CSV et le convertit en DataFrame
    # DataFrame = structure de donnÃ©es 2D (comme une table SQL)
    # Exemple :
    # | Timestamp        | EventId | SourceIP | User | Raw_Message |
    # |------------------|---------|----------|------|-------------|
    # | 2024-12-10...    | E27     | 173.2... | None | reverse ... |
    
    # ======== LIGNE 2 : VÃ‰RIFICATION COLONNE ========
    if 'Timestamp' in df.columns:
        # VÃ©rifier que la colonne 'Timestamp' existe
        # Si elle n'existe pas, on ne peut pas faire l'analyse temporelle
        
        # ======== LIGNE 3 : CONVERSION DE DATES ========
        df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')
        # Convertit la colonne texte '2024-12-10 06:55:46' en objet datetime
        # errors='coerce' = si une date est mal formÃ©e, la remplacer par NaT (Not a Time)
        # Pourquoi important ? Permet les comparaisons : date1 > date2 (sinon erreur)
        
    else:
        # Si colonne Timestamp n'existe pas
        st.error("Erreur: Le fichier CSV doit contenir une colonne 'Timestamp'.")
        # Affiche un message d'erreur en rouge dans l'interface
        return pd.DataFrame()
        # Retourne un DataFrame vide (signale une erreur)
    
    # ======== LIGNE 4 : RETOUR ========
    return df
    # Retourne le DataFrame nettoyÃ© et prÃªt Ã  l'emploi
```

### 4.4 Fonction Principale (Interface)

```python
# ======== DÃ‰FINITION DE LA FONCTION PRINCIPALE ========
def main():
    # Toute la logique de l'interface est encapsulÃ©e dans cette fonction
    # C'est un pattern propre en Python
    
    # ======== TITRE ========
    st.title("ğŸ”’ Dashboard de SÃ©curitÃ© : Clinique Tamalou")
    # CrÃ©e un titre de niveau 1 (Ã©quivalent Ã  <h1> en HTML)
    # L'emoji ğŸ”’ donne un aspect "sÃ©curitÃ©"
    
    # ======== SIDEBAR - UPLOAD DE FICHIER ========
    st.sidebar.header("ğŸ“ DonnÃ©es")
    # "st.sidebar" = tout ce qui suit apparaÃ®t dans la barre latÃ©rale gauche
    # header() = titre de section
    
    uploaded_file = st.sidebar.file_uploader(
        "Charger un nouveau fichier CSV",
        # Label du bouton
        type=['csv']
        # Restriction : seuls les fichiers .csv sont acceptÃ©s
    )
    # file_uploader() retourne un objet fichier (ou None si rien n'est uploadÃ©)
    
    # ======== LOGIQUE DE CHARGEMENT ========
    if uploaded_file is not None:
        # Si l'utilisateur A uploadÃ© un fichier personnalisÃ©
        st.sidebar.success("Fichier personnalisÃ© chargÃ© !")
        # Message de succÃ¨s (en vert)
        df_brut = load_data(uploaded_file)
        # Charge le fichier uploadÃ©
        
    else:
        # Si l'utilisateur N'a RIEN uploadÃ©
        try:
            # "try" = essayer d'exÃ©cuter le code suivant
            # "except" = si une erreur surgit, faire quelque chose
            
            df_brut = load_data('datasetssh.csv')
            # Charge le fichier de dÃ©mo par dÃ©faut
            st.sidebar.info("Utilisation du fichier de dÃ©mo par dÃ©faut.")
            # Message informatif (en bleu)
            
        except FileNotFoundError:
            # Si 'datasetssh.csv' n'existe pas
            st.error("Fichier de dÃ©mo 'datasetssh.csv' introuvable.")
            return
            # ArrÃªte l'exÃ©cution (pas de donnÃ©es = pas d'interface)
    
    # ======== SÃ‰CURITÃ‰ : DATAFRAME VIDE ? ========
    if df_brut.empty:
        # Si le DataFrame est vide (pas une seule ligne)
        return
        # ArrÃªte tout (erreur critique)
    
    # ======== SIDEBAR - FILTRES ========
    st.sidebar.header("Filtres")
    # Nouveau titre de section dans la sidebar
    
    # -------- FILTRE 1 : DATES --------
    # On rÃ©cupÃ¨re les dates min/max du dataset pour les bornes
    min_date = df_brut['Timestamp'].min()
    # min() retourne la date la plus ANCIENNE
    max_date = df_brut['Timestamp'].max()
    # max() retourne la date la plus RÃ‰CENTE
    
    start_date = st.sidebar.date_input(
        "Date de dÃ©but",
        # Label
        min_date,
        # Valeur par dÃ©faut (premiÃ¨re date du dataset)
        min_value=min_date,
        # L'utilisateur ne peut pas aller AVANT cette date
        max_value=max_date
        # L'utilisateur ne peut pas aller APRÃˆS cette date
    )
    # Retourne un objet date (ex : datetime.date(2024, 12, 10))
    
    end_date = st.sidebar.date_input(
        "Date de fin",
        max_date,
        min_value=min_date,
        max_value=max_date
    )
    
    # -------- FILTRE 2 : EVENT ID (Type d'attaque) --------
    all_event_ids = df_brut['EventId'].unique()
    # unique() retourne les valeurs UNIQUES (sans doublon) de la colonne
    # Exemple : ['E27', 'E13', 'E12', 'E21', ...] (environ 10 types d'Ã©vÃ©nements)
    
    selected_events = st.sidebar.multiselect(
        "SÃ©lectionner les EventId",
        # Label
        all_event_ids,
        # Liste des options disponibles
        default=all_event_ids
        # Par dÃ©faut, TOUS les EventId sont sÃ©lectionnÃ©s
    )
    # Retourne une LISTE des valeurs cochÃ©es par l'utilisateur
    # Exemple : ['E27', 'E13', 'E12'] (l'utilisateur a dÃ©crochÃ© E21)
    
    # -------- FILTRE 3 : IP SPÃ‰CIFIQUE --------
    all_ips = df_brut['SourceIP'].unique()
    # Liste de toutes les IPs du dataset (ex: ['173.234.31.186', '183.129.154.138', ...])
    
    selected_ip = st.sidebar.selectbox(
        "Rechercher une IP spÃ©cifique",
        # Label
        options=["Toutes"] + list(all_ips)
        # OPTIONS = ["Toutes", '173.234.31.186', '183.129.154.138', ...]
        # "Toutes" permet de ne PAS filtrer par IP
    )
    # Retourne UNE SEULE valeur sÃ©lectionnÃ©e par l'utilisateur (pas une liste)
    # Exemple : "173.234.31.186" ou "Toutes"
    
    # ======== APPLICATION DES FILTRES ========
    # CrÃ©er des "masques boolÃ©ens" (True/False pour chaque ligne)
    
    # -------- MASQUE 1 : DATES --------
    mask_date = (df_brut['Timestamp'].dt.date >= start_date) & (df_brut['Timestamp'].dt.date <= end_date)
    # df_brut['Timestamp'].dt.date = extrait JUSTE la date (pas l'heure)
    # >= start_date : lignes APRÃˆS la date de dÃ©but
    # <= end_date : lignes AVANT la date de fin
    # & = ET logique (les deux conditions doivent Ãªtre vraies)
    # Exemple :
    #   Ligne 1 : Timestamp = 2024-12-10, start = 2024-12-10, end = 2025-01-07 â†’ TRUE (incluse)
    #   Ligne 2 : Timestamp = 2024-12-09, start = 2024-12-10, end = 2025-01-07 â†’ FALSE (exclue)
    
    # -------- MASQUE 2 : EVENEMENT --------
    mask_event = df_brut['EventId'].isin(selected_events)
    # isin() = "est dans la liste ?"
    # Exemple :
    #   Ligne 1 : EventId = E27, selected_events = ['E27', 'E13'] â†’ TRUE (E27 est dans la liste)
    #   Ligne 2 : EventId = E21, selected_events = ['E27', 'E13'] â†’ FALSE (E21 n'est pas dans la liste)
    
    # -------- COMBINATION DES DEUX MASQUES --------
    df_filtered = df_brut[mask_date & mask_event]
    # & = ET logique : la ligne doit respecter BOTH conditions pour Ãªtre incluse
    # Exemple :
    #   Ligne 1 : mask_date=TRUE, mask_event=TRUE â†’ incluse
    #   Ligne 2 : mask_date=TRUE, mask_event=FALSE â†’ exclue
    #   Ligne 3 : mask_date=FALSE, mask_event=TRUE â†’ exclue
    
    # -------- MASQUE 3 : IP (OPTIONNEL) --------
    if selected_ip != "Toutes":
        # Si l'utilisateur A sÃ©lectionnÃ© une IP spÃ©cifique (pas "Toutes")
        df_filtered = df_filtered[df_filtered['SourceIP'] == selected_ip]
        # Filtre ENCORE le DataFrame pour garder que cette IP
    
    # Ã€ ce stade, df_filtered contient UNIQUEMENT les lignes qui respectent TOUS les filtres
    
    # ======== AFFICHAGE DES RÃ‰SULTATS ========
    st.markdown("---")
    # Affiche une ligne horizontale (sÃ©parateur visuel)
    
    # -------- SÃ‰CURITÃ‰ : TABLEAU VIDE ? --------
    if df_filtered.empty:
        st.warning("âš ï¸ Aucune donnÃ©e ne correspond Ã  vos filtres.")
        # Si aucune ligne ne correspond, affiche un message d'alerte (jaune)
    else:
        # Sinon (si des donnÃ©es existent)
        
        # -------- KPIs (INDICATEURS CLÃ‰S) --------
        col1, col2, col3 = st.columns(3)
        # CrÃ©e 3 colonnes de largeur Ã©gale cÃ´te Ã  cÃ´te
        
        with col1:
            # Contenu DANS la premiÃ¨re colonne
            st.metric(
                "Total Logs (FiltrÃ©s)",
                # Label
                df_filtered.shape[0]
                # shape[0] = nombre de LIGNES
                # Exemple : 655147
            )
        
        with col2:
            # Contenu DANS la deuxiÃ¨me colonne
            pourcentage = (len(df_filtered) / len(df_brut)) * 100
            # len() = compte le nombre de lignes
            # len(df_filtered) / len(df_brut) = ratio
            # * 100 = conversion en pourcentage
            # Exemple : 655147 / 655147 * 100 = 100.0%
            st.metric("% du Dataset", f"{pourcentage:.1f}%")
            # f"...{pourcentage:.1f}%" = formatage chaÃ®ne
            # :.1f = affiche 1 seul chiffre aprÃ¨s la virgule
            # Exemple : 100.0% (pas 100.000001%)
        
        with col3:
            # Contenu DANS la troisiÃ¨me colonne
            st.metric(
                "IPs Uniques",
                df_filtered['SourceIP'].nunique()
                # nunique() = nombre de VALEURS UNIQUES
                # Exemple : 1129 IPs diffÃ©rentes
            )
        
        # -------- TABLEAU DE DONNÃ‰ES --------
        st.subheader("ğŸ“‹ Logs FiltrÃ©s")
        # Sous-titre
        st.dataframe(
            df_filtered,
            # Le DataFrame Ã  afficher
            use_container_width=True
            # Le tableau utilise 100% de la largeur disponible
        )
        # Affiche un tableau interactif (scrollable, triable par colonne)
        
        # -------- VISUALISATIONS --------
        st.markdown("---")
        st.header("ğŸ“Š Analyse Visuelle")
        
        # CrÃ©e 2 colonnes pour 2 graphiques cÃ´te Ã  cÃ´te
        chart_col1, chart_col2 = st.columns(2)
        
        # -------- GRAPHIQUE 1 : TOP 10 IPS --------
        with chart_col1:
            st.subheader("Top 10 IPs Attaquantes")
            top_ips = df_filtered['SourceIP'].value_counts().head(10)
            # value_counts() = compte les occurrences de chaque valeur
            # Exemple rÃ©sultat :
            #   173.234.31.186      45000
            #   183.129.154.138     40000
            #   ...
            # head(10) = garder seulement les 10 premiÃ¨res
            st.bar_chart(top_ips)
            # Affiche un bar chart (graphique en barres)
        
        # -------- GRAPHIQUE 2 : TIME SERIES --------
        with chart_col2:
            st.subheader("Volume d'attaques par Heure")
            time_series = df_filtered.set_index('Timestamp').resample('H').size()
            # set_index('Timestamp') = utilise Timestamp comme index (pour le regroupement temporel)
            # resample('H') = regroupe par HEURE (H = hour)
            # .size() = compte le nombre de lignes dans chaque groupe
            # Exemple rÃ©sultat :
            #   2024-12-10 00:00:00    1200 (1200 attaques entre 00h et 01h)
            #   2024-12-10 01:00:00    950
            #   ...
            st.line_chart(time_series)
            # Affiche un line chart (graphique en courbe)
        
        # -------- GRAPHIQUE 3 : TOP USERNAMES --------
        st.markdown("---")
        st.subheader("ğŸš¨ Top Usernames TentÃ©s")
        top_users = df_filtered['User'].value_counts().head(10)
        # MÃªme logique que top_ips (mais avec les noms d'utilisateurs)
        # Exemple : root (400k tentatives), admin (5k), support (200), ...
        st.bar_chart(top_users)

# ======== POINT D'ENTRÃ‰E ========
if __name__ == "__main__":
    # Cette condition = "si ce fichier est lancÃ© directement (pas importÃ© ailleurs)"
    main()
    # Appelle la fonction principale
```

---

## 5. DÃ‰PLOIEMENT ET PRODUCTION

### 5.1 PrÃ©paration GitHub

**Commandes Git (dans le dossier ssh_monitor) :**

```bash
# 1. Initialiser Git localement
git init
# CrÃ©e un dÃ©pÃ´t Git cachÃ© (.git)

# 2. Ajouter tous les fichiers
git add .
# Stocke les fichiers modifiÃ©s en "staging area"

# 3. CrÃ©er un commit (snapshot)
git commit -m "Ajout Bonus Upload + Version Finale"
# Sauvegarde les changements avec un message descriptif

# 4. Envoyer sur GitHub
git push
# Synchronise le dÃ©pÃ´t local avec GitHub (si un remote est configurÃ©)
```

**Structure du commit :**
```
ssh_monitor/
â”œâ”€â”€ app.py (code source)
â”œâ”€â”€ requirements.txt (dÃ©pendances)
â”œâ”€â”€ .gitignore (fichiers ignorÃ©s)
â”œâ”€â”€ datasetssh.csv (donnÃ©es)
â””â”€â”€ ... (autres fichiers)
```

### 5.2 DÃ©ploiement sur Streamlit Cloud

1. Aller sur https://share.streamlit.io/
2. Cliquer "New app"
3. Remplir le formulaire :
   - **Repository :** Yassine-Bouzidi/Analyse-Logs-SSH
   - **Branch :** main
   - **Main file path :** project/ssh_monitor/app.py
   - **App URL :** dashboard-ssh-ysn
4. Cliquer "Deploy!"

**RÃ©sultat :** L'app est en ligne Ã  https://dashboard-ssh-ysn.streamlit.app

### 5.3 DÃ©ploiement Continu (CI/CD)

Chaque fois que vous faites `git push` :
1. GitHub reÃ§oit le nouveau code
2. Streamlit Cloud dÃ©tecte le changement (via webhook)
3. L'app est reconstruite automatiquement (environ 1 min)
4. La version en ligne se met Ã  jour

C'est l'avantage du dÃ©ploiement continu : zÃ©ro downtime, mise Ã  jour instantanÃ©e.

---

## 6. RÃ‰SULTATS ET MÃ‰TRIQUES

### Performance
- **Temps de chargement initial :** ~3 secondes (premiÃ¨re fois)
- **Temps de chargement aprÃ¨s cache :** <100ms (5Ã¨me clic)
- **Volume de donnÃ©es traitÃ© :** 655 147 logs
- **Nombre de colonnes :** 5 (Timestamp, EventId, SourceIP, User, Raw_Message)
- **Nombre d'IPs uniques :** 1 129
- **Nombre d'EventId uniques :** ~10 types d'attaques

### Utilisation RÃ©elle
- **Filtrage par date :** RÃ©duit le dataset de 10% Ã  100% selon la plage
- **Filtrage par EventId :** RÃ©duit de 5% Ã  100%
- **Filtrage par IP :** RÃ©duit de 0.1% Ã  5%
- **Combinaison des filtres :** RÃ©duit le dataset de faÃ§on EXPONENTIELLE

Exemple : (50% des dates) Ã— (30% des EventId) Ã— (0.5% d'une IP) = 0.075% du dataset original

---

## 7. AMÃ‰LIORATIONS FUTURES (Roadmap)

### V2 : FonctionnalitÃ©s AvancÃ©es
1. **GÃ©olocalisation** : Afficher une carte avec les adresses IP et leurs localisations
2. **Export PDF** : Bouton pour tÃ©lÃ©charger un rapport filtrÃ©
3. **Authentification** : SÃ©curiser l'accÃ¨s avec login/password (via `st.secrets`)
4. **Real-time Updates** : IntÃ©gration avec une API pour les logs live
5. **Alertes** : Notifications email si une IP dÃ©passe X tentatives
6. **Machine Learning** : DÃ©tection d'anomalies (comportement anormal)

### Infrastructure
1. **Base de donnÃ©es** : Passer de CSV Ã  PostgreSQL pour plus de performance
2. **ScalabilitÃ©** : Migrer de Streamlit Cloud vers Kubernetes (si trafic augmente)
3. **Backup** : Sauvegardes automatiques des logs en cloud

---

## 8. CONCLUSION

Ce projet dÃ©montre une **maÃ®trise complÃ¨te du cycle de dÃ©veloppement** :
- âœ… Analyse de donnÃ©es (Pandas, ETL)
- âœ… DÃ©veloppement d'interface (Streamlit)
- âœ… DÃ©ploiement en production (GitHub, Cloud)
- âœ… Optimisation (Cache, Performance)
- âœ… Documentation et communication

Le livrable est **prÃªt pour un SOC (Security Operations Center) rÃ©el** et peut accÃ©lÃ©rer la dÃ©tection de menaces.

---

## ANNEXES

### Glossaire Technique
- **ETL** : Extract (extraire), Transform (transformer), Load (charger)
- **DataFrame** : Table de donnÃ©es en mÃ©moire (colonne + lignes)
- **Cache** : Stockage temporaire en mÃ©moire pour Ã©viter recalcul
- **Masque boolÃ©en** : Tableau True/False pour filtrer les lignes
- **CI/CD** : Continuous Integration / Continuous Deployment (automatisation)
- **SOC** : Security Operations Center (Ã©quipe de sÃ©curitÃ©)

### Ressources
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Pandas User Guide](https://pandas.pydata.org/docs/)
- [Git Tutorial](https://git-scm.com/doc)
- [GitHub Pages](https://github.com/)

---

**Fin du rapport**

*Document crÃ©Ã© le 8 DÃ©cembre 2025 - Yassine Bouzidi*