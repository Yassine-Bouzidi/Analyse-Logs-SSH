# Rapport d'Apprentissage Python - Brief Pipeline d'Analyse de Logs SSH

---

## üìö Contexte du Projet

Dans le cadre de ma formation en cybers√©curit√©, j'ai d√©velopp√© un pipeline complet d'analyse de logs SSH pour d√©tecter et visualiser des attaques par force brute sur un serveur critique. Ce projet m'a permis d'acqu√©rir des comp√©tences en Python pour le traitement de donn√©es et la cr√©ation de rapports SOC.

---

## üéØ Objectifs d'Apprentissage

L'objectif principal √©tait de ma√Ætriser la cha√Æne de traitement de la donn√©e de s√©curit√© : **Collecte ‚Üí Parsing ‚Üí Normalisation ‚Üí Analyse ‚Üí Reporting**.

### Comp√©tences Techniques Vis√©es
1. Manipulation de fichiers texte et CSV en Python
2. Utilisation des expressions r√©guli√®res (regex) pour le parsing
3. Analyse de donn√©es avec Pandas
4. Cr√©ation de visualisations avec Matplotlib/Seaborn
5. Utilisation de Jupyter Notebook pour la pr√©sentation d'analyses

### Comp√©tences SOC
1. Identification de patterns d'attaques SSH
2. Analyse de logs de s√©curit√©
3. Cr√©ation de rapports d'incident
4. Formulation de recommandations de durcissement

---

## üöÄ Fonctionnalit√©s Cl√©s

- **Parsing Avanc√©** : Analyse de 55 types d'√©v√©nements SSH diff√©rents (authentification, erreurs r√©seau, tentatives d'intrusion).
- **Gestion Intelligente des Dates** : D√©tection automatique des changements d'ann√©e (ex: passage de d√©c. 2024 √† janv. 2025) pour une chronologie exacte, m√™me sur des logs √† cheval sur deux ann√©es.
- **Z√©ro Faux N√©gatifs** : Syst√®me d'identification exhaustif atteignant **0% de logs "UNKNOWN"** gr√¢ce √† un moteur d'expressions r√©guli√®res (Regex) it√©ratif et optimis√©.
- **D√©tection des Menaces** : Identification pr√©cise des attaques par force brute, des utilisateurs invalides et des anomalies de protocole (ex: *Bad packet length*, *Corrupted MAC*).
- **Rapports Automatis√©s** : G√©n√©ration de rapports CSV d√©taill√©s pour l'analyse approfondie et d'un r√©sum√© ex√©cutif affich√© en console avec les statistiques cl√©s (Top attaquants, volum√©trie).

---

## üìñ Apprentissages Python par Phase

### Phase 1 : ETL et Scripting Python

#### Concepts Appris
**1. Manipulation de Fichiers**
Lecture de fichiers ligne par ligne:

with open('SSH.txt', 'r', encoding='utf-8') as f:
for line in f:


# Traitement
**Apprentissage** : J'ai d√©couvert l'importance du gestionnaire de contexte `with` pour g√©rer automatiquement la fermeture des fichiers et √©viter les fuites m√©moire.

**2. Expressions R√©guli√®res (Regex)**
import re
ip_pattern = r'from\s+(\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3})'
match = re.search(ip_pattern, message)

**Apprentissage** : Les regex sont puissantes pour extraire des patterns dans du texte non structur√©. J'ai appris :
- Les groupes de capture `( )`
- Les quantificateurs `+`, `*`, `{n,m}`
- Les raw strings `r''` pour √©viter l'√©chappement excessif
- La diff√©rence entre `.search()` et `.match()`

**3. Structures de Donn√©es**
Dictionnaires pour organiser les donn√©es
event_patterns = {
'E9': r'Failed password for (?!invalid user).+',
'E10': r'Failed password for invalid user .+'
}

**Apprentissage** : Les dictionnaires Python sont id√©aux pour mapper des cl√©s (EventId) √† des valeurs (patterns regex). J'ai compris l'importance de structurer les donn√©es d√®s la conception.

**4. Module CSV**
import csv
with open('output.csv', 'w', newline='', encoding='utf-8') as f:
writer = csv.DictWriter(f, fieldnames=['Timestamp', 'EventId'])
writer.writeheader()
writer.writerows(results)

**Apprentissage** : Le param√®tre `newline=''` est crucial sous Windows pour √©viter les lignes vides. J'ai appris la diff√©rence entre `csv.writer` et `csv.DictWriter`.

**5. Gestion des Dates**
from datetime import datetime
timestamp = datetime.strptime('Dec 10 06:55:46 2025', '%b %d %H:%M:%S %Y')

**Apprentissage** : Les formats de date peuvent √™tre complexes. J'ai d√©couvert les directives `strptime` (%b, %d, %H, etc.) pour parser des dates en diff√©rents formats.

#### Difficult√©s Rencontr√©es

**Probl√®me 1 : FileNotFoundError**
- **Erreur** : Le script ne trouvait pas `SSH.txt` malgr√© sa pr√©sence
- **Cause** : Diff√©rence entre le r√©pertoire de travail et l'emplacement du fichier
- **Solution** : Utilisation de `os.path.dirname(__file__)` pour des chemins relatifs robustes

**Probl√®me 2 : SyntaxError avec raw strings**
- **Erreur** : `unterminated string literal` avec les chemins Windows
- **Cause** : Raw strings ne peuvent pas se terminer par un backslash seul
- **Solution** : Utilisation de slashes `/` ou double backslash `\\`

**Probl√®me 3 : Patterns Regex trop g√©n√©riques**
- **Erreur** : EventId E9 capturait aussi E10
- **Cause** : Regex mal ordonn√©es et patterns qui se chevauchent
- **Solution** : Ordre de priorit√© explicite et negative lookahead `(?!invalid user)`

**Probl√®me 4 : Persistance de logs "UNKNOWN"**
- **Erreur** : 1957 lignes (0.3%) class√©es comme inconnues, masquant potentiellement des attaques.
- **Cause** : Patterns regex trop stricts (ex: espaces manquants) et √©v√©nements non pr√©vus (erreurs Java/JCraft, probl√®mes de protocole).
- **Solution** : 
  1. Analyse it√©rative des messages rejet√©s.
  2. Cr√©ation de 20 nouveaux patterns (E28-E55).
  3. Flexibilisation des regex avec `\s+`.
  4. R√©sultat : **0 log inconnu (100% de couverture)**.

---

### Phase 2 : Jupyter Notebook et Environnement de Travail

#### Concepts Appris

**1. Installation de Packages**
python -m pip install pandas matplotlib seaborn jupyter


**Apprentissage** : J'ai compris la diff√©rence entre `pip install` et `python -m pip install` (plus fiable pour garantir l'installation dans le bon environnement Python).

**2. Cellules Markdown vs Code**

**Apprentissage** : Jupyter permet d'alterner entre :
- **Cellules Markdown** : Documentation, titres, explications (storytelling)
- **Cellules Code** : Ex√©cution de Python avec r√©sultats affich√©s

Cette s√©paration est id√©ale pour cr√©er des rapports SOC lisibles et reproductibles.

**3. Kernels et Environnements**

**Apprentissage** : VS Code peut avoir plusieurs environnements Python. J'ai appris √† :
- V√©rifier le kernel actif en haut √† droite du notebook
- Installer les packages dans le bon environnement
- Utiliser `!pip install` directement dans une cellule Jupyter

#### Difficult√©s Rencontr√©es

**Probl√®me 1 : ModuleNotFoundError pour pandas**
- **Erreur** : `No module named 'pandas'`
- **Cause** : Packages install√©s dans un environnement Python diff√©rent du kernel Jupyter
- **Solution** : Installation dans le kernel actif ou s√©lection du bon kernel

**Probl√®me 2 : Cellule Markdown interpr√©t√©e comme Code**
- **Erreur** : `SyntaxError: invalid syntax` sur du texte Markdown
- **Cause** : Type de cellule incorrect (Python au lieu de Markdown)
- **Solution** : Conversion en Markdown avec le menu d√©roulant ou raccourci `M`

---

### Phase 3 : Analyse de Donn√©es avec Pandas et Visualisation

#### Concepts Appris

**1. DataFrames Pandas**
df = pd.read_csv('datasetssh.csv')
df.head() # Aper√ßu
df.shape # Dimensions
df.info() # Types de colonnes

**Apprentissage** : Les DataFrames sont comme des tableaux Excel en Python. J'ai d√©couvert des m√©thodes puissantes pour explorer rapidement les donn√©es.

**2. Conversion de Types**
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
df['Hour'] = df['Timestamp'].dt.hour

**Apprentissage** : Pandas g√®re automatiquement les dates avec `.dt` accessor. Cela simplifie √©norm√©ment l'analyse temporelle.

**3. Agr√©gations et Comptages**
top_ips = df['SourceIP'].value_counts().head(5)
root_attacks = df[df['User'] == 'root'].shape

**Apprentissage** : `.value_counts()` est incroyablement utile pour compter les occurrences. Le filtrage avec `df[condition]` est intuitif.

**4. Visualisations avec Matplotlib**
plt.figure(figsize=(14, 6))
plt.bar(x, y, color='crimson')
plt.xlabel('Label')
plt.title('Titre')
plt.show()

**Apprentissage** : Matplotlib suit une logique de "peinture" successive. J'ai appris √† :
- D√©finir la taille avec `figsize`
- Personnaliser les couleurs et styles
- Ajouter des labels et titres
- Afficher avec `.show()`

**5. Pie Charts**
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)

**Apprentissage** : Les pie charts sont efficaces pour montrer des proportions. Le param√®tre `autopct` affiche automatiquement les pourcentages.

#### Difficult√©s Rencontr√©es

**Probl√®me 1 : Valeurs NaN dans les colonnes**
- **Erreur** : Certaines IPs ou utilisateurs √©taient `NaN`
- **Cause** : Regex ne captaient pas tous les formats de logs
- **Solution** : Patterns regex am√©lior√©s et gestion des cas manquants

**Probl√®me 2 : Graphiques pas affich√©s dans Jupyter**
- **Erreur** : Code ex√©cut√© mais pas de graphique visible
- **Cause** : Oubli de `plt.show()` ou mauvaise configuration du backend
- **Solution** : Ajout syst√©matique de `.show()` et configuration inline

---

## üõ†Ô∏è Ressources Utilis√©es

### Documentation Officielle
1. **Python Docs** : https://docs.python.org/3/
   - Module `re` (regex)
   - Module `csv`
   - Module `datetime`

2. **Pandas Documentation** : https://pandas.pydata.org/docs/
   - Getting Started Guide
   - API Reference pour DataFrame
   - Time Series / Date functionality

3. **Matplotlib Documentation** : https://matplotlib.org/stable/
   - Pyplot Tutorial
   - Gallery d'exemples

4. **Jupyter Documentation** : https://jupyter-notebook.readthedocs.io/
   - Installation guide
   - Markdown cells

### Tutoriels et Articles
1. **Real Python** : Tutoriels sur regex, pandas, et matplotlib
2. **Stack Overflow** : R√©solution de probl√®mes sp√©cifiques (FileNotFoundError, SyntaxError)
3. **GeeksforGeeks** : Exemples de code pour value_counts(), to_datetime()

### Outils de D√©veloppement
1. **VS Code** : √âditeur avec extension Jupyter
2. **PowerShell** : Terminal pour ex√©cution des scripts
3. **Git** : Versioning du code (recommand√© pour la suite)

---

## üí° Comp√©tences Acquises

### Comp√©tences Techniques Python

| Comp√©tence |
|------------|
| Manipulation de fichiers | 
| Expressions r√©guli√®res |
| Pandas DataFrames |
| Visualisation (Matplotlib) |
| Jupyter Notebook |

### Comp√©tences SOC

- **Analyse de logs** : Capacit√© √† parser et interpr√©ter des logs SSH
- **D√©tection de menaces** : Identification de patterns d'attaques par force brute
- **Reporting** : Cr√©ation de rapports visuels pour stakeholders
- **Recommandations** : Formulation de mesures de durcissement pertinentes

---

## üîÑ M√©thodologie de Travail

### Approche It√©rative

J'ai adopt√© une approche **it√©rative** pour ce projet :

1. **Version 1** : Script basique qui lit les fichiers
2. **Version 2** : Ajout du parsing regex simple
3. **Version 3** : Am√©lioration des patterns et gestion des erreurs
4. **Version 4** : Optimisation et ajout de statistiques
5. **Version 5** : Int√©gration Jupyter et visualisations

### Debugging M√©thodique

Pour chaque erreur rencontr√©e :
1. **Lecture du message d'erreur** (ligne, type d'erreur)
2. **Isolation du probl√®me** (test sur un √©chantillon r√©duit)
3. **Recherche de solutions** (documentation, Stack Overflow)
4. **Test de la correction**
5. **Documentation de la solution** (commentaires dans le code)

### Test et Validation

- Test sur les **5 premi√®res lignes** avant de traiter l'ensemble
- V√©rification des **outputs interm√©diaires** (print statements)
- Validation des **r√©sultats** avec des calculs manuels sur un √©chantillon

---

## üìà √âvolution et Prochaines √âtapes

### Points Forts du Projet
‚úÖ Pipeline ETL fonctionnel et robuste  
‚úÖ Code bien comment√© et structur√©  
‚úÖ Visualisations claires et pertinentes  
‚úÖ Rapports exploitables pour SOC  

### Axes d'Am√©lioration
üîÑ Ajouter des tests unitaires (pytest)  
üîÑ G√©rer des formats de logs multiples (Apache, Nginx, etc.)  
üîÑ Cr√©er une interface web (Flask/Streamlit)  
üîÑ Automatiser avec des scripts cron  
üîÑ Int√©grer avec des SIEM (Splunk, ELK)  

### Comp√©tences √† Approfondir
üìö Machine Learning pour d√©tection d'anomalies  
üìö API REST pour exposer les r√©sultats  
üìö Docker pour conteneurisation du pipeline  
üìö Bases de donn√©es (PostgreSQL) pour stocker les logs  

---

## CODE D√âTAILL√â AVEC EXPLICATIONS

### Initialisation et Chargement
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Imports : Tu charges pandas (pour manipuler les tableaux de donn√©es), matplotlib et seaborn (pour cr√©er les graphiques).
# Warnings : warnings.filterwarnings('ignore') est une astuce pour garder ton notebook propre. Cela emp√™che Python d'afficher des avertissements rouges (souvent li√©s √† des mises √† jour futures de biblioth√®ques) qui ne bloquent pas le code mais polluent l'affichage.
```

```python
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', None)
df = pd.read_csv('data/datasetssh.csv')
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)
print(f"üìä Dataset : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\n")

# Settings : Tu configures pandas pour afficher jusqu'√† 100 lignes et toutes les colonnes (utile si ton √©cran est large). Tu d√©finis aussi la taille par d√©faut des graphiques (12x6).
# Chargement : read_csv charge ton fichier de logs dans la variable df (DataFrame). C'est maintenant ton tableau Excel virtuel.
# sns.set_style('whitegrid') : Ajoute une grille blanche en fond de graphique (plus lisible et pro).
# figure.figsize : D√©finit une taille d'image par d√©faut (12x6) assez large pour √™tre lisible.
# V√©rification : df.shape te donne imm√©diatement la taille (nombre de lignes = nombre de logs, colonnes = infos extraites). C'est le premier r√©flexe d'un Data Analyst pour v√©rifier que le chargement a fonctionn√©.
```

### Inspection des Anomalies (Unknown)
```python
unknown_logs = df[df['EventId'] == 'UNKNOWN']

# Filtrage : Tu cr√©es un sous-tableau unknown_logs qui ne contient que les lignes o√π l'ID de l'√©v√©nement n'a pas √©t√© reconnu par ton script pr√©c√©dent.
# Investigation : Si tu en as (len > 0), tu affiches les messages bruts (Raw_Message) et les IPs sources.
# Pourquoi c'est important ? En cybers√©curit√©, un log "inconnu" peut √™tre soit une erreur de parsing (ton script a mal lu la ligne), soit une nouvelle m√©thode d'attaque que tu ne connais pas encore.
```

### Traitement Temporel (Time Series)

```python
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
df['Hour'] = df['Timestamp'].dt.hour
df['Minute'] = df['Timestamp'].dt.minute
df['Date'] = df['Timestamp'].dt.date

# Conversion : Par d√©faut, les dates dans un CSV sont lues comme du texte. pd.to_datetime les convertit en objets datetime intelligents.
# Extraction : Tu cr√©es de nouvelles colonnes (Hour, Minute) pour faciliter l'analyse par heure plus tard. Cela te permettra de r√©pondre √† la question : "√Ä quelle heure les pirates attaquent-ils le plus ?".
```

###  Analyse des Attaquants (Top IPs)

```python
top_ips = df['SourceIP'].value_counts().head(5)

# Comptage : value_counts() compte combien de fois chaque IP appara√Æt et les trie par ordre d√©croissant.
# Calcul de fr√©quence : (count / len(df)) * 100 calcule quel pourcentage du trafic total repr√©sente chaque attaquant. C'est vital pour prioriser : si une seule IP fait 43% des attaques, c'est ta cible prioritaire √† bloquer.
```

### Analyse des Cibles (Utilisateurs & Root)

```python
root_attacks = df[df['User'] == 'root'].shape[0]
total_with_user = df['User'].notna().sum()

# Focus Root : Tu filtres le tableau pour ne garder que les lignes o√π l'utilisateur est "root". .shape[0] compte le nombre de lignes r√©sultantes.
# Remplissage : df['User'].notna().sum() compte combien de logs ont un utilisateur identifi√© (contrairement √† ceux o√π c'est juste une connexion technique sans user).
```

### Analyse des Types d'Attaques (EventId)

```python
event_counts = df['EventId'].value_counts()

# Distribution : Permet de voir comment on t'attaque. Est-ce surtout du "Failed password" (force brute) ou du "Invalid user" (dictionnaire d'utilisateurs) ?
# Visualisation texte : La ligne bar = "‚ñà" * int(percentage) est une astuce sympa pour faire un mini-graphique directement dans la console textuelle.
```

### Visualisation Graphique (Data Viz)

**Graphique 1 : Bar Chart (Top IPs)**
```python
bars = plt.bar(range(len(top_10_ips)), top_10_ips.values, color='crimson'...)

# Choix du graph : Un diagramme en barres est id√©al pour comparer des quantit√©s (nombre d'attaques) entre diff√©rentes cat√©gories (IPs).
# Couleur : 'crimson' (rouge sang) est choisi pour rappeler le danger/l'alerte.
# Annotations : La boucle for avec plt.text ajoute le chiffre exact au-dessus de chaque barre, ce qui rend le graph lisible m√™me sans regarder l'axe Y.
```

**Graphique 2 : Pie Chart (Camembert des √©v√©nements)**
```python
if others > 0:
    pie_data = pd.concat([top_5_events, pd.Series({'Autres': others})])
else:
    pie_data = top_5_events

wedges, texts, autotexts = plt.pie(pie_data.values, labels=pie_data.index, ...)

# La Pr√©paration (pd.concat): C'est l'√©tape de calcul. Python trie tes donn√©es brutes (qui sont trop nombreuses et illisibles) pour cr√©er un petit groupe propre : les 5 √©v√©nements principaux + une cat√©gorie "Autres".

# R√©sultat : Une liste de donn√©es pr√™te √† l'emploi.

# La Pr√©sentation (plt.pie et explode): C'est l'√©tape de dessin. Python prend la liste propre pr√©par√©e juste avant et g√©n√®re l'image du camembert. C'est ici qu'on ajoute l'option explode pour √©carter les parts et rendre le graphique joli.

# R√©sultat : L'image finale du graphique.
```

**Graphique 3 : Line Chart (Chronologie)**
```python
attacks_per_hour = df.groupby('Hour').size()
plt.plot(..., color='darkred')
plt.fill_between(...)

# Groupby : Tu regroupes les donn√©es par heure (0h, 1h... 23h) et tu comptes la taille (size) de chaque groupe.

# Rendu : fill_between colorie la zone sous la courbe, ce qui donne un effet de volume √† l'attaque. Cela permet de voir s'il y a eu un pic soudain (attaque script√©e massive) ou si c'est constant.
```

## üéì Conclusion

Ce brief m'a permis de d√©velopper des comp√©tences solides en Python pour l'analyse de donn√©es de s√©curit√©. J'ai appris √† :

1. **Automatiser** des t√¢ches r√©p√©titives de parsing de logs
2. **Structurer** des donn√©es non structur√©es avec des regex
3. **Analyser** des volumes importants de donn√©es avec Pandas
4. **Communiquer** des r√©sultats techniques via des visualisations
---
